{"cells":[{"cell_type":"markdown","source":["This file: Extract standardized GPT classification result from JSONL Raw output file\n","\n","Input:\n","- 2_batch_api_output.jsonl: GPT raw output JSONL file\n","- 0_investment_sentences.csv: original sentence level dataset\n","\n","Output:\n","- 3_gpt_result_1.csv: std GPT classification result\n","- 3_gpt_result_2.csv: std GPT classification result, mapped back to sentence texts"],"metadata":{"id":"lw5GGXO_00SK"},"id":"lw5GGXO_00SK"},{"cell_type":"code","execution_count":1,"id":"5d78e7c8","metadata":{"id":"5d78e7c8","executionInfo":{"status":"ok","timestamp":1747863954023,"user_tz":-120,"elapsed":481,"user":{"displayName":"Yiyang Wu","userId":"14558260507896847088"}}},"outputs":[],"source":["import json\n","import csv\n","import re\n","import pandas as pd\n","import os"]},{"cell_type":"markdown","source":["Notice: If you're using colab, run the following two cells"],"metadata":{"id":"QQIntWWX0xjy"},"id":"QQIntWWX0xjy"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a56b54EZznp6","executionInfo":{"status":"ok","timestamp":1747863978068,"user_tz":-120,"elapsed":23315,"user":{"displayName":"Yiyang Wu","userId":"14558260507896847088"}},"outputId":"1fae5790-e154-4ff8-95bc-84cd74b7c9cc"},"id":"a56b54EZznp6","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["os.chdir('/content/drive/MyDrive/Colab Notebooks/acct4_ta_s1')"],"metadata":{"id":"J4gJlHcczoOU","executionInfo":{"status":"ok","timestamp":1747863990513,"user_tz":-120,"elapsed":41,"user":{"displayName":"Yiyang Wu","userId":"14558260507896847088"}}},"id":"J4gJlHcczoOU","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Set Up File Path"],"metadata":{"id":"77krjvzF1Zjl"},"id":"77krjvzF1Zjl"},{"cell_type":"code","execution_count":7,"id":"65abb29a","metadata":{"id":"65abb29a","executionInfo":{"status":"ok","timestamp":1747864070959,"user_tz":-120,"elapsed":6,"user":{"displayName":"Yiyang Wu","userId":"14558260507896847088"}}},"outputs":[],"source":["input_file = \"2_batch_api_output.jsonl\"\n","raw_file = \"0_investment_sentences.csv\"\n","output_file_1 = \"3_gpt_result_1.csv\"\n","output_file_2 = \"3_gpt_result_2.csv\""]},{"cell_type":"markdown","source":["# Functions to extract std answer from JSON text"],"metadata":{"id":"wr6bVmwj1c_4"},"id":"wr6bVmwj1c_4"},{"cell_type":"code","execution_count":8,"id":"cf21803c","metadata":{"id":"cf21803c","executionInfo":{"status":"ok","timestamp":1747864072041,"user_tz":-120,"elapsed":15,"user":{"displayName":"Yiyang Wu","userId":"14558260507896847088"}}},"outputs":[],"source":["def extract_json_from_text(text):\n","    # Find JSON content within the triple backticks\n","    match = re.search(r'```json\\s*(.*?)\\s*```', text, re.DOTALL)\n","    if match:\n","        return match.group(1)\n","    return None"]},{"cell_type":"code","source":["def process_jsonl_file(input_file, output_file):\n","    with open(input_file, 'r', encoding='utf-8') as f_in, \\\n","         open(output_file, 'w', newline='', encoding='utf-8') as f_out:\n","\n","        fieldnames = [\n","            'id',\n","            'invest_plan',\n","            'invest_target',\n","            'invest_amount'\n","        ]\n","        writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n","        writer.writeheader()\n","\n","        error_lines = []\n","\n","        for line in f_in:\n","            try:\n","                line_data = json.loads(line)\n","                summary_text = (\n","                    line_data.get('response', {})\n","                             .get('body', {})\n","                             .get('choices', [])[0]\n","                             .get('message', {})\n","                             .get('content', '')\n","                )\n","\n","                # Try to extract JSON inside triple backticks\n","                json_str = extract_json_from_text(summary_text)\n","                if not json_str:\n","                    # Fallback to raw content\n","                    json_str = summary_text.strip()\n","                    # Strip wrapping quotes if any\n","                    if json_str.startswith('\"') and json_str.endswith('\"'):\n","                        json_str = json_str[1:-1]\n","                    # Unescape common escapes\n","                    json_str = json_str.replace('\\\\\"', '\"').replace('\"\"', '\"')\n","\n","                data = json.loads(json_str)\n","\n","                row = {\n","                    'id': data.get('id'),\n","                    'invest_plan': data.get('invest_plan'),\n","                    'invest_target': data.get('invest_target'),\n","                    'invest_amount': data.get('invest_amount')\n","                }\n","                writer.writerow(row)\n","\n","            except Exception as e:\n","                print(f\"Error processing line: {e}\")\n","                error_lines.append({\n","                    'id': f\"ERROR_{len(error_lines)}\",\n","                    'invest_plan': line.strip(),\n","                    'invest_target': \"\",\n","                    'invest_amount': \"\"\n","                })\n","\n","        # append any error rows\n","        for err in error_lines:\n","            writer.writerow(err)"],"metadata":{"id":"QpXg5_HH0XNq"},"id":"QpXg5_HH0XNq","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Function to map gpt classification result back to original text dataset"],"metadata":{"id":"AwBzC_NQ1i-9"},"id":"AwBzC_NQ1i-9"},{"cell_type":"code","source":["def combine_with_raw_og_data(output_file_1, raw_file, output_file):\n","\n","    output_df = pd.read_csv(output_file_1)\n","    output_df['id'] = output_df['id'].astype(str)\n","\n","    raw_sentence = pd.read_csv(raw_file)\n","\n","    # Create a dictionary to map sentence_id to sentence\n","    raw_sentence['id'] = raw_sentence['sentence_id'].astype(str)\n","    guid_to_text = dict(zip(raw_sentence['id'], raw_sentence['sentence']))\n","\n","    # Add a new column for the original text\n","    output_df['Original_Text'] = output_df['id'].map(guid_to_text)\n","\n","    # Save to csv file\n","    output_df.to_csv(output_file, index=False)\n","    print(f\"Combined data saved to {output_file}\")"],"metadata":{"id":"0YpPGRZh0ZUb"},"id":"0YpPGRZh0ZUb","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":9,"id":"63e0c120","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63e0c120","executionInfo":{"status":"ok","timestamp":1747864074792,"user_tz":-120,"elapsed":460,"user":{"displayName":"Yiyang Wu","userId":"14558260507896847088"}},"outputId":"aee1b562-4246-4cc1-bd7d-16a4882a81e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data extracted and saved to 3_gpt_result_1.csv\n","Combined data saved to 3_gpt_result_2.csv\n"]}],"source":["process_jsonl_file(input_file, output_file_1)\n","print(f\"Data extracted and saved to {output_file_1}\")\n","\n","combine_with_raw_og_data(output_file_1, raw_file, output_file_2)"]}],"metadata":{"kernelspec":{"display_name":"openai","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}